{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4211d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "````xml\n",
    "<!-- filepath: /home/paul/Cours/BGDIA708/Projet-IA-Fairness/Grad-Eclip-main/grad_eclip_text_explanation.ipynb -->\n",
    "<VSCode.Cell id=\"text_exp_intro\" language=\"markdown\">\n",
    "# Grad-ECLIP : Explications textuelles - Analyse de l'influence des mots\n",
    "\n",
    "Ce notebook implémente une extension de **Grad-ECLIP** pour analyser l'influence de chaque mot du prompt textuel sur la correspondance vision-texte dans les modèles comme CLIP.\n",
    "\n",
    "## Différence avec l'analyse spatiale\n",
    "\n",
    "- **Analyse spatiale** (notebook précédent) : \"Quelles parties de l'image sont importantes ?\"\n",
    "- **Analyse textuelle** (ce notebook) : \"Quels mots du prompt sont les plus influents ?\"\n",
    "\n",
    "## Applications pratiques\n",
    "\n",
    "1. **Débuggage de prompts** : Identifier les mots parasites ou peu pertinents\n",
    "2. **Optimisation de requêtes** : Comprendre quels termes améliorent la recherche\n",
    "3. **Analyse de biais** : Détecter si certains mots introduisent des biais indésirables\n",
    "4. **Conception d'interfaces** : Mettre en évidence les mots-clés importants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75952d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances\n",
    "# ...existing code... (same as image notebook)\n",
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606aea1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Compose, Resize, ToTensor, Normalize, InterpolationMode\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclip\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/_meta_registrations.py:163\u001b[39m\n\u001b[32m    153\u001b[39m     torch._check(\n\u001b[32m    154\u001b[39m         grad.dtype == rois.dtype,\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m    158\u001b[39m         ),\n\u001b[32m    159\u001b[39m     )\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad.new_empty((batch_size, channels, height, width))\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;129m@torch\u001b[39m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_fake\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtorchvision::nms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[32m    165\u001b[39m     torch._check(dets.dim() == \u001b[32m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    166\u001b[39m     torch._check(dets.size(\u001b[32m1\u001b[39m) == \u001b[32m4\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mboxes should have 4 elements in dimension 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets.size(\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, InterpolationMode\n",
    "import clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clipmodel, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "print(f\"Modèle chargé sur : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965c19a",
   "metadata": {},
   "source": [
    "## Méthode : Grad-ECLIP pour l'analyse textuelle\n",
    "\n",
    "### Principe théorique\n",
    "\n",
    "Au lieu d'analyser les gradients par rapport aux features visuelles, nous calculons les gradients par rapport aux **embeddings de tokens textuels**.\n",
    "\n",
    "#### Étapes clés :\n",
    "\n",
    "1. **Tokenisation** : Décomposition du texte en tokens\n",
    "2. **Embeddings textuels** : Conversion des tokens en vecteurs \n",
    "3. **Calcul de gradients** : ∇(similarité) par rapport aux embeddings textuels\n",
    "4. **Attribution d'importance** : Score d'influence pour chaque token\n",
    "\n",
    "#### Formule d'attribution :\n",
    "`````\n",
    "<userPrompt>\n",
    "Provide the fully rewritten file, incorporating the suggested code change. You must produce the complete file.\n",
    "</userPrompt>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
