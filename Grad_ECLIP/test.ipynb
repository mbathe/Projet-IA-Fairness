{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3064f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def pre_caption(caption, max_words=50):\n",
    "    caption = re.sub(\n",
    "        r\"([.!\\\"()*#:;~])\",       \n",
    "        ' ',\n",
    "        caption.lower(),\n",
    "    )\n",
    "    caption = re.sub(\n",
    "        r\"\\s{2,}\",\n",
    "        ' ',\n",
    "        caption,\n",
    "    )\n",
    "    caption = caption.rstrip('\\n') \n",
    "    caption = caption.strip(' ')\n",
    "\n",
    "    #truncate caption\n",
    "    caption_words = caption.split(' ')\n",
    "    if len(caption_words)>max_words:\n",
    "        caption = ' '.join(caption_words[:max_words])\n",
    "            \n",
    "    return caption\n",
    "    \n",
    "def generate_hm(hm_type, img, txt_embedding, txts, resize):\n",
    "    start = time.time()\n",
    "    img_keepsized = imgprocess_keepsize(img).to(device).unsqueeze(0)\n",
    "    outputs, v_final, last_input, v, q_out, k_out,\\\n",
    "        attn, att_output, map_size = clip_encode_dense(img_keepsized)\n",
    "    img_embedding = F.normalize(outputs[:,0], dim=-1)\n",
    "    cosines = (img_embedding @ txt_embedding.T)[0]\n",
    "\n",
    "    if hm_type == \"selfattn\":\n",
    "        emap = attn[0,:1,1:].detach().reshape(*map_size)\n",
    "    elif \"gradcam\" in hm_type:\n",
    "        emap = [grad_cam(c, last_input, map_size) for c in cosines]\n",
    "        emap = torch.stack(emap, dim=0).sum(0)\n",
    "    elif \"maskclip\" in hm_type:\n",
    "        emap = mask_clip(txt_embedding.T, v_final, k_out, map_size)\n",
    "        emap = emap.sum(0)\n",
    "    elif \"eclip\" in hm_type:\n",
    "        emap = [grad_eclip(c, q_out, k_out, v, att_output, map_size, withksim=False) \\\n",
    "            if \"wo-ksim\" in hm_type else grad_eclip(c, q_out, k_out, v, att_output, map_size, withksim=True) \\\n",
    "            for c in cosines]\n",
    "        emap = torch.stack(emap, dim=0).sum(0)  \n",
    "   \n",
    "    \n",
    "    print(\"processing time: \", end-start)\n",
    "    \n",
    "    emap -= emap.min()\n",
    "    emap /= emap.max()\n",
    "    emap = resize(emap.unsqueeze(0))[0]\n",
    "    return emap\n",
    "\n",
    "def visualize(hmap, raw_image, resize):\n",
    "    image = np.asarray(raw_image.copy())\n",
    "    hmap = resize(hmap.unsqueeze(0))[0].cpu().numpy()\n",
    "    color = cv2.applyColorMap((hmap*255).astype(np.uint8), cv2.COLORMAP_JET) # cv2 to plt\n",
    "    color = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)\n",
    "    c_ret = np.clip(image * (1 - 0.5) + color * 0.5, 0, 255).astype(np.uint8)\n",
    "    return c_ret"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
