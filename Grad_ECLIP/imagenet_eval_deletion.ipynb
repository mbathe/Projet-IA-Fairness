{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4182ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClipWrapper(\n",
       "  (vision_model): image_encoder_wrapper(\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embeddings): VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    )\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_model): text_encoder_wrapper(\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embeddings): TextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "    )\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from clip import tokenize\n",
    "from clip_utils import build_zero_shot_classifier\n",
    "from imagenet_metadata import IMAGENET_CLASSNAMES, OPENAI_IMAGENET_TEMPLATES\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.transforms import Resize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from generate_emap import clipmodel, preprocess, imgprocess_keepsize, mm_clipmodel, mm_interpret, \\\n",
    "    clip_encode_dense, grad_eclip, grad_cam, mask_clip, compute_rollout_attention, surgery_model, clip_surgery_map, \\\n",
    "    m2ib_model, m2ib_clip_map\n",
    "\n",
    "import Game_MM_CLIP.clip as mm_clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mm_clipmodel.to(device)\n",
    "clipmodel.to(device)\n",
    "surgery_model.to(device)\n",
    "m2ib_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed24e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    # print(\"pred:\", pred.shape) # [5, 10] \n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    acc = [correct[:k,:].float().sum(0).cpu() for k in topk]\n",
    "    pred_top1 = pred[0,:]\n",
    "    return acc, pred_top1\n",
    "\n",
    "def make_grids(h, w):\n",
    "    shifts_x = torch.arange(\n",
    "        0, w, 1)\n",
    "    shifts_y = torch.arange(\n",
    "        0, h, 1)\n",
    "    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x, indexing='ij')\n",
    "    shift_x = shift_x.reshape(-1)\n",
    "    shift_y = shift_y.reshape(-1)\n",
    "    grids = torch.stack((shift_x, shift_y), dim=1)\n",
    "    return grids\n",
    "\n",
    "def random_pixel(image, poses):\n",
    "    # adjust the size of perturbation each step based on the size of object size \n",
    "    h,w, _ = image.shape \n",
    "    random_patch = torch.rand(len(poses), 3) * 255.\n",
    "    xs, ys = zip(*poses)\n",
    "    image[ys, xs, :] = random_patch\n",
    "    return image\n",
    "\n",
    "def delection_process(image, heatmap, L, cal_gap, del_path, img_name):\n",
    "    image_array = np.array(image).copy()\n",
    "    image_array.setflags(write=1)\n",
    "    h, w = heatmap.shape\n",
    "    grids = make_grids(h, w)\n",
    "    order = np.argsort(-heatmap.reshape(-1))\n",
    "    area = h*w\n",
    "    pixel_once = max(1, int(area/(2*L)))\n",
    "\n",
    "    del_imgs = []\n",
    "    for step in range(1,L+1):\n",
    "        image_array = random_pixel(image_array, grids[order[(step-1)*pixel_once:step*pixel_once]])\n",
    "        if step%cal_gap == 0:\n",
    "            pil_image = Image.fromarray(np.uint8(image_array))\n",
    "            img_clipreprocess = preprocess(pil_image).to(device).unsqueeze(0)\n",
    "            del_imgs.append(img_clipreprocess)\n",
    "            # pil_image.save(del_path+'/{}_{}.jpg'.format(img_name, step))\n",
    "    return torch.cat(del_imgs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5579b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hm(hm_type, img, gt, pred, resize):\n",
    "    img_keepsized = imgprocess_keepsize(img).to(device).unsqueeze(0)\n",
    "    outputs, v_final, last_input, v, q_out, k_out,\\\n",
    "        attn, att_output, map_size = clip_encode_dense(img_keepsized)\n",
    "    img_embedding = F.normalize(outputs[:,0], dim=-1)\n",
    "    if \"gt\" in hm_type:\n",
    "        exp_target = gt\n",
    "        txt_embedding = zero_shot_weights[:, gt]\n",
    "        cosine = (img_embedding @ txt_embedding)[0]\n",
    "    elif \"pred\" in hm_type:\n",
    "        exp_target = pred\n",
    "        txt_embedding = zero_shot_weights[:, pred]\n",
    "        cosine = (img_embedding @ txt_embedding)[0]\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    if hm_type == \"selfattn\":\n",
    "        emap = attn[0,:1,1:].detach().reshape(*map_size)\n",
    "    elif \"gradcam\" in hm_type:\n",
    "        emap = grad_cam(cosine, last_input, map_size)\n",
    "    elif \"maskclip\" in hm_type:\n",
    "        emap = mask_clip(txt_embedding.unsqueeze(-1), v_final, k_out, map_size)[0]\n",
    "    elif \"eclip\" in hm_type:\n",
    "        emap = grad_eclip(cosine, q_out, k_out, v, att_output, map_size, withksim=False) \\\n",
    "            if \"wo-ksim\" in hm_type else grad_eclip(cosine, q_out, k_out, v, att_output, map_size, withksim=True)\n",
    "    elif \"game\" in hm_type:\n",
    "        img_clipreprocess = preprocess(img).to(device).unsqueeze(0)\n",
    "        text_tokenized = mm_clip.tokenize(IMAGENET_CLASSNAMES[exp_target]).to(device)\n",
    "        emap = mm_interpret(model=mm_clipmodel, image=img_clipreprocess, texts=text_tokenized, device=device)[0]\n",
    "    elif \"rollout\" in hm_type:\n",
    "        img_clipreprocess = preprocess(img).to(device).unsqueeze(0)\n",
    "        text_tokenized = mm_clip.tokenize(IMAGENET_CLASSNAMES[exp_target]).to(device)   \n",
    "        attentions = mm_interpret(model=mm_clipmodel, image=img_clipreprocess, texts=text_tokenized, device=device, rollout=True)      \n",
    "        emap = compute_rollout_attention(attentions)[0]\n",
    "    elif \"surgery\" in hm_type:\n",
    "        img_clipreprocess = preprocess(img).to(device).unsqueeze(0)\n",
    "        all_texts = ['airplane', 'bag', 'bed', 'bedclothes', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'building', 'bus', 'cabinet', 'car', 'cat', 'ceiling', 'chair', 'cloth', 'computer', 'cow', 'cup', 'curtain', 'dog', 'door', 'fence', 'floor', 'flower', 'food', 'grass', 'ground', 'horse', 'keyboard', 'light', 'motorbike', 'mountain', 'mouse', 'person', 'plate', 'platform', 'potted plant', 'road', 'rock', 'sheep', 'shelves', 'sidewalk', 'sign', 'sky', 'snow', 'sofa', 'table', 'track', 'train', 'tree', 'truck', 'tv monitor', 'wall', 'water', 'window', 'wood']\n",
    "        all_texts.insert(0, IMAGENET_CLASSNAMES[exp_target])\n",
    "        emap = clip_surgery_map(model=surgery_model, image=img_clipreprocess, texts=all_texts, device=device)[0,:,:,0]\n",
    "    elif \"m2ib\" in hm_type:\n",
    "        img_clipreprocess = preprocess(img).to(device).unsqueeze(0)\n",
    "        emap = m2ib_clip_map(model=m2ib_model, image=img_clipreprocess, texts=IMAGENET_CLASSNAMES[exp_target], device=device)\n",
    "        emap = torch.tensor(emap)\n",
    "\n",
    "\n",
    "    emap -= emap.min()\n",
    "    emap /= emap.max()\n",
    "    emap = resize(emap.unsqueeze(0))[0].cpu().numpy()\n",
    "    return emap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82f826b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 49.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[class name embeddings]: torch.Size([512, 1000])\n",
      "Evaluating rollout_gt...\n",
      "Start: Processing the 0th folder, target class name: tench\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/infres/pmbathe-24/Projet-IA-Fairness/Grad_ECLIP/data/val/n01440764'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m folder = values[\u001b[32m0\u001b[39m]\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStart: Processing the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33mth folder, target class name: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(label, IMAGENET_CLASSNAMES[label]))\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m files = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m+\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m     67\u001b[39m     img_name = f.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/infres/pmbathe-24/Projet-IA-Fairness/Grad_ECLIP/data/val/n01440764'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"/home/infres/pmbathe-24/Projet-IA-Fairness/Grad_ECLIP/data/val/\"\n",
    "ins_path = '/home/infres/pmbathe-24/Projet-IA-Fairness/Grad_ECLIP/data/ins_samples/'  ### for debug, saving insertion samples\n",
    "\n",
    "zero_shot_weights = build_zero_shot_classifier(\n",
    "    clipmodel,\n",
    "    classnames=IMAGENET_CLASSNAMES,\n",
    "    templates=OPENAI_IMAGENET_TEMPLATES,\n",
    "    num_classes_per_batch=10,\n",
    "    device=device,\n",
    "    use_tqdm=True\n",
    "    )\n",
    "print(\"[class name embeddings]:\", zero_shot_weights.shape)  # [512, 1000]\n",
    "\n",
    "L = 100\n",
    "cal_gap = 10\n",
    "\n",
    "# Define all heatmap types to evaluate\n",
    "hm_types = ['rollout', 'eclip','game', 'maskclip', 'gradcam', 'clipsurgery','m2ib']\n",
    "\n",
    "extensions = [\"gt\", \"pred\"]\n",
    "\n",
    "NUM_CLASSES_TO_PROCESS = 5  # Number of classes in ImageNet to process\n",
    "\n",
    "# Method name mapping for display\n",
    "method_names = {\n",
    "    'gradcam': 'Grad-CAM', \n",
    "    'maskclip': 'MaskCLIP',\n",
    "     'eclip': 'Grad-ECLIP',\n",
    "    'game': 'GAME',\n",
    "    'rollout': 'Rollout',\n",
    "    'clipsurgery': 'CLIPSurgery',\n",
    "    'm2ib': 'M2IB',\n",
    "}\n",
    "\n",
    "# Storage for results\n",
    "all_results = {}\n",
    "insertion_curves = {}\n",
    "\n",
    "for ex in extensions: \n",
    "    for hm_type_old in hm_types:\n",
    "        hm_type = f\"{hm_type_old}_{ex}\"\n",
    "        print(f\"Evaluating {hm_type}...\")\n",
    "        \n",
    "        # Initialize metrics for each insertion step\n",
    "        top1 = torch.zeros([11])  # 11 steps: 100, 90, 80, ..., 0\n",
    "        top5 = torch.zeros([11])\n",
    "        top10 = torch.zeros([11])\n",
    "        n = torch.zeros([11])\n",
    "        \n",
    "        # Storage for insertion curves\n",
    "        insertion_steps = list(range(0, L + 1, cal_gap))\n",
    "\n",
    "        with open(\"imagenet_class_index.json\", \"r\") as ff:\n",
    "            class_dict = json.load(ff)\n",
    "            for label, values in list(class_dict.items())[0:NUM_CLASSES_TO_PROCESS]:  # process first 100 classes\n",
    "                label = int(label)\n",
    "                folder = values[0]\n",
    "                print(\"Start: Processing the {}th folder, target class name: {}\".format(label, IMAGENET_CLASSNAMES[label]))\n",
    "\n",
    "                files = os.listdir(data_path+folder)\n",
    "\n",
    "                for f in files:\n",
    "                    img_name = f.split(\".\")[0]\n",
    "                    img = Image.open(os.path.join(data_path, folder, f)).convert(\"RGB\")\n",
    "                    w, h = img.size\n",
    "                    # in case there is too large image\n",
    "                    if min(w,h) > 640:\n",
    "                        scale = min(w,h) / 640\n",
    "                        hs = int(h/scale)\n",
    "                        ws = int(w/scale)\n",
    "                        img = img.resize((ws,hs))\n",
    "                    w, h = img.size\n",
    "                    resize = Resize((h,w))\n",
    "                    \n",
    "                    # make prediction\n",
    "                    with torch.no_grad():\n",
    "                        img_clipreprocess = preprocess(img).to(device).unsqueeze(0)\n",
    "                        img_clip_embedding = clipmodel.encode_image(img_clipreprocess)\n",
    "                        logits = 100. * img_clip_embedding @ zero_shot_weights\n",
    "                        target = torch.tensor([label]).to(device)\n",
    "                        [acc1, acc5, acc10], pred_top1 = accuracy(logits, target, topk=(1, 5, 10))\n",
    "                        top1[:1] += acc1\n",
    "                        top5[:1] += acc5\n",
    "                        top10[:1] += acc10\n",
    "                        n[0] += 1\n",
    "\n",
    "                    hm = generate_hm(hm_type, img.copy(), label, pred_top1.item(), resize)\n",
    "                    ins_imgs = delection_process(img, hm, L, cal_gap, ins_path+folder, '{}_{}'.format(img_name, hm_type))\n",
    "                    \n",
    "                    # Evaluate insertion steps\n",
    "                    img_clip_embedding = clipmodel.encode_image(ins_imgs)\n",
    "                    logits = 100. * img_clip_embedding @ zero_shot_weights\n",
    "                    target = torch.tensor([label]).repeat(len(img_clip_embedding)).to(device)\n",
    "                    [acc1, acc5, acc10], _ = accuracy(logits, target, topk=(1, 5, 10))\n",
    "                    top1[1:] += acc1\n",
    "                    top5[1:] += acc5\n",
    "                    top10[1:] += acc10\n",
    "                    n[1:] += 1\n",
    "                    \n",
    "        # Calculate final accuracies\n",
    "        top1_acc = (top1 / n).numpy()\n",
    "        top5_acc = (top5 / n).numpy()\n",
    "        top10_acc = (top10 / n).numpy()\n",
    "        \n",
    "        # Store results for this method\n",
    "        if ex == \"gt\":\n",
    "            gt_suffix = \"_gt\"\n",
    "        else:\n",
    "            gt_suffix = \"_pred\" \n",
    "            \n",
    "        base_method = hm_type_old\n",
    "        \n",
    "        if base_method not in all_results:\n",
    "            all_results[base_method] = {}\n",
    "            \n",
    "        # Store final accuracy (at step 100)\n",
    "        all_results[base_method][f'{ex}_top1'] = top1_acc[-1]  # Last step (100%)\n",
    "        all_results[base_method][f'{ex}_top5'] = top5_acc[-1]\n",
    "        \n",
    "        # Store insertion curves\n",
    "        insertion_curves[hm_type] = {\n",
    "            'steps': insertion_steps,\n",
    "            'top1': top1_acc,\n",
    "            'top5': top5_acc,\n",
    "            'top10': top10_acc\n",
    "        }\n",
    "\n",
    "# Create results table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INSERTION EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<15} {'Ground-truth':<20} {'Prediction':<20}\")\n",
    "print(f\"{'':15} {'@1':<10} {'@5':<10} {'@1':<10} {'@5':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for method in hm_types:\n",
    "    if method in all_results:\n",
    "        method_name = method_names.get(method, method)\n",
    "        results = all_results[method]\n",
    "        gt_top1 = results.get('gt_top1', 0)\n",
    "        gt_top5 = results.get('gt_top5', 0)\n",
    "        pred_top1 = results.get('pred_top1', 0)\n",
    "        pred_top5 = results.get('pred_top5', 0)\n",
    "        \n",
    "        print(f\"{method_name:<15} {gt_top1:<10.4f} {gt_top5:<10.4f} \"\n",
    "              f\"{pred_top1:<10.4f} {pred_top5:<10.4f}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_data = []\n",
    "for method in hm_types:\n",
    "    if method in all_results:\n",
    "        method_name = method_names.get(method, method)\n",
    "        results = all_results[method]\n",
    "        results_data.append({\n",
    "            'Method': method_name,\n",
    "            'GT_Top1': results.get('gt_top1', 0),\n",
    "            'GT_Top5': results.get('gt_top5', 0),\n",
    "            'Pred_Top1': results.get('pred_top1', 0),\n",
    "            'Pred_Top5': results.get('pred_top5', 0)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv('insertion_evaluation_results.csv', index=False)\n",
    "print(f\"\\nResults saved to insertion_evaluation_results.csv\")\n",
    "\n",
    "# Plot insertion curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Ground-truth curves\n",
    "for method in hm_types:\n",
    "    gt_key = f\"{method}_gt\"\n",
    "    pred_key = f\"{method}_pred\"\n",
    "    \n",
    "    if gt_key in insertion_curves:\n",
    "        curve_data = insertion_curves[gt_key]\n",
    "        method_name = method_names.get(method, method)\n",
    "        \n",
    "        # GT Top-1\n",
    "        ax1.plot(curve_data['steps'], curve_data['top1'], \n",
    "                label=method_name, marker='o', markersize=4)\n",
    "        \n",
    "        # GT Top-5\n",
    "        ax2.plot(curve_data['steps'], curve_data['top5'], \n",
    "                label=method_name, marker='o', markersize=4)\n",
    "\n",
    "# Prediction curves\n",
    "for method in hm_types:\n",
    "    pred_key = f\"{method}_pred\"\n",
    "    \n",
    "    if pred_key in insertion_curves:\n",
    "        curve_data = insertion_curves[pred_key]\n",
    "        method_name = method_names.get(method, method)\n",
    "        \n",
    "        # Pred Top-1\n",
    "        ax3.plot(curve_data['steps'], curve_data['top1'], \n",
    "                label=method_name, marker='o', markersize=4)\n",
    "        \n",
    "        # Pred Top-5\n",
    "        ax4.plot(curve_data['steps'], curve_data['top5'], \n",
    "                label=method_name, marker='o', markersize=4)\n",
    "\n",
    "# Configure subplots\n",
    "ax1.set_title('Ground-truth Top-1 Deletion')\n",
    "ax1.set_xlabel('Deletion Step')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_title('Ground-truth Top-5 Deletion')\n",
    "ax2.set_xlabel('Deletion Step')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3.set_title('Prediction Top-1 Deletion')\n",
    "ax3.set_xlabel('Deletion Step')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4.set_title('Prediction Top-5 Deletion')\n",
    "ax4.set_xlabel('Deletion Step')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('insertion_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInsertion curves saved to insertion_curves_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df704f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
